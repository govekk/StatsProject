---
title: "Results Section"
author: "Matt Carter & Kiya Govek & James Yang"
date: "2/19/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE, cache = FALSE}
library(ggplot2)
library(knitr)
library(tidyr)
library(forcats)
library(gridExtra)

opts_template$set(figure1 = list(fig.height = 3, fig.width = 5.5),
                  figure2 = list(fig.height = 3, fig.width = 5.5),
                  figure3 = list(fig.height = 4.5, fig.width = 5.5)) 
```


**Results**


In this section, we examine the four estimators (Method of Moment, Maximum Likelihood, and two of our custom estimators) by looking at the results of our simulation study. We have computed the mean squared error, variance as well bias for each of our estimator, given different sample sizes and population sizes as the input. Different inputs of sample size and population size are used to test if the performances of the estimators are consistent. We plot the properties of the estimators in the line charts displayed below.


```{r, echo = FALSE}
set.seed(23)

# function that takes input for sample size, population size, whether or not to 
# include table, number of graph, and runs simulation for our four estimators and
# draw a line chart describing the properties (MSE, bias, variance) of the estimators

plot_simulation <- function(N_input, k_input, table = FALSE, num) {
  
  # run simulation given sample size and population size
  N <- N_input
  k <- k_input
  runs <- 10000
  mom_statistic <- numeric(runs)
  mle_statistic <- numeric(runs)
  custom1_statistic <- numeric(runs)
  custom2_statistic <- numeric(runs)
  
  for(i in 1:runs) {
    sample_values <- sample(1:N, k)
    mom_statistic[i] <- 2*mean(sample_values) - 1
    mle_statistic[i] <- max(sample_values)
    custom1_statistic[i] <- max(sample_values) + 
      (max(sample_values)^(k+1) + k)/((k+1)*max(sample_values)^k)
    custom2_statistic[i] <- max(sample_values) + max(sample_values)/(k+1)
  }
  
  # functions that calculate MSE and Bias of estimators
  mse <- function(statistic) {
    return(var(statistic) + (mean(statistic) - N)^2)
  }
  bias <- function(statistic) {
    return(mean(statistic) - N)
  }
  
  # calculate properties of estimators
  stats <- list(mom_statistic, mle_statistic, custom1_statistic, custom2_statistic)
  mse_list <- unlist(lapply(stats, FUN = mse))
  var_list <- unlist(lapply(stats, FUN = var))
  bias_list <- unlist(lapply(stats, FUN = bias))
  names <- c("MoM", "MLE", "Custom 1", "Custom 2")
  
  # put into data frame and gather the estimator values
  params_data <- data.frame(Estimators = names, MSE = mse_list, 
                       Variance = var_list, Bias = bias_list)
  params <- gather(params_data, key = stat, value = Value, MSE, Variance, Bias)

  # plot the line graph
  plt <- ggplot(data = params, aes(x = fct_inorder(Estimators), y = Value)) +
    geom_line(aes(group = fct_inorder(stat), color = fct_inorder(stat))) + 
    geom_label(aes(label = round(Value))) +
    xlab("Estimators") +
    labs(color = "Properties") +
    ggtitle(paste0("Figure ", num, ". ",
                  "Sample Size ", k_input, 
                  " and Population Size ", N_input))
  
  # plot table if input requests so
  if (table) {
    params_rounded <- data.frame(Estimators = names, MSE = round(mse_list, digits = 2), 
                                 Variance = round(var_list, digits = 2), 
                                 Bias = round(bias_list, digits = 2))
    tt <- ttheme_default(colhead=list(fg_params = list(parse=TRUE)))
    tbl <- tableGrob(params_rounded, rows=NULL, theme=tt)
    # plot chart and table into one object
    grid.arrange(plt, tbl,
                nrow=2,
                as.table=TRUE,
                heights=c(2,1))
  }
  else {plt}
}
```


```{r, echo = FALSE, opts.label = "figure1"}
plot_simulation(N_input = 20, k_input = 5, num = 1)
```

From `Figure 1`, we see that our first and second custom estimators are equally the best estimators in this case, since they both have the least mean squared errors and bias of 0. However, MLE has the least variance thus most efficient, but since its bias is significantly larger than that of the other three, its mean squared error is not optimal. In comparison, the method of moment estimator is much worse, given its much higher variance. 


```{r, echo = FALSE, opts.label = "figure2"}
plot_simulation(200, 30, num = 2)
```

In `Figure 2`, after we increase the sample size and population size, we see that the trend between the four estimators is roughly the same as `Figure 1`, except that the mean squared error and variance of the method of moment estimator is much larger than those of the rest, which increases the slope of the line chart. Our two custom estimators still have the best performances among the four, in terms of MSE. 


```{r, echo = FALSE, warning = FALSE, opts.label = "figure3"}
plot_simulation(2000, 100, table = TRUE, num = 3)
```

In `Figure 3`, we find that as sizes increase, the MSE of MoM estimator becomes significantly worse. Our MLE and second estimator are similar in terms of efficiency, but MLE has a worse bias thus MSE. Unfortunately, we get `NA` values for our first custom estimator, because in order to get the estimate, we need to first derive $max(sample\_values)^{(k+1)}$, which exceeds the limit of R's calculation. Given that our two custom estimators have similar performances given smaller sample sizes, and our second estimator has a consistently great performance, unlike our first one, which crashes under large sample/population sizes, we decide to use our second custom estimator given its low mean squared error compared to MoM and MLE, and its consistency in calculations compared to our first custom estimator. 


```{r, echo = FALSE}
# conduct bootstrap test and calculate confidence interval for the given sample

sample_values <- c(922, 299, 1106, 121, 1621, 1164, 1912, 937, 914, 593, 
                85, 1090, 1004, 139, 1451, 24, 267, 1045, 1062, 1274)

n <- length(sample_values)       # sample size
N <- 10^4                # desired no. resamples

estimator <- max(sample_values) + max(sample_values)/(n+1) 

# conduct bootstrap test
boot_statistic <- numeric(N)
for (i in 1:N) {
  x <- sample(sample_values, size = n, replace = TRUE)
  boot_statistic[i] <- max(x) + max(x)/(n+1)
}

# calculate bootstrap confidence interval
conf <- quantile(boot_statistic, probs = c(0.025, 0.975))
```

After deciding the estimator, we use it to estimate $N$ based on a random sample of 20 objects[^1]. Our estimate for $N$ is `r round(estimator)` after rounding. We also perform bootstrap on the random sample, and compute the 95% bootstrap confidence interval for our estimate. We are 95% confident that the true $N$ is between `r round(conf[[1]], digits = 2)` and `r round(conf[[2]], digits = 2)`. 

[^1]:
The random sample used above consists of `r sample_values`. 